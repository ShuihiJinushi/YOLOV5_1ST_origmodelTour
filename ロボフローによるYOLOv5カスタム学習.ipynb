{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrsaDfdVHzxt"
   },
   "source": [
    "# Custom Training with YOLOv5\n",
    "\n",
    "In this tutorial, we assemble a dataset and train a custom YOLOv5 model to recognize the objects in our dataset. To do so we will take the following steps:\n",
    "\n",
    "* Gather a dataset of images and label our dataset\n",
    "* Export our dataset to YOLOv5\n",
    "* Train YOLOv5 to recognize the objects in our dataset\n",
    "* Evaluate our YOLOv5 model's performance\n",
    "* Run test inference to view our model at work\n",
    "\n",
    "\n",
    "\n",
    "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv5でカスタムデータ学習をする\n",
    "\n",
    "このチュートリアルでは、データセットを集め独自のYOLOv5 modelに自分のデータセットを認識させてみます。<br>\n",
    "そのためには次のステップを実行します:\n",
    "\n",
    "* 画像データセットを集めそのデータセットにラベル付けをする\n",
    "* データセットをYOLOv5に対してエクスポートする\n",
    "* YOLOv5を学習させて用意したデータセットの物体を認識させる\n",
    "* 作ったYOLOv5 modelのパフォーマンスを評価する\n",
    "* テスト推論を実行し、新しいモデルの動作を見る\n",
    "\n",
    "\n",
    "\n",
    "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNveqeA1KXGy"
   },
   "source": [
    "# Step 1: Install Requirements（必要なインストール）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTvDNSILZoN9",
    "outputId": "bba69fdb-d2ca-4dc4-d5b9-847c176f7612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.12.1+cpu (CPU)\n"
     ]
    }
   ],
   "source": [
    "##clone YOLOv5 and \n",
    "#!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "#%cd yolov5\n",
    "#%pip install -qr requirements.txt # install dependencies\n",
    "#%pip install -q roboflow\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP6USLgz2f0r"
   },
   "source": [
    "# Step 2: Assemble Our Dataset\n",
    "\n",
    "In order to train our custom model, we need to assemble a dataset of representative images with bounding box annotations around the objects that we want to detect. And we need our dataset to be in YOLOv5 format.\n",
    "\n",
    "In Roboflow, you can choose between two paths:\n",
    "\n",
    "* Convert an existing dataset to YOLOv5 format. Roboflow supports over [30 formats object detection formats](https://roboflow.com/formats) for conversion.\n",
    "* Upload raw images and annotate them in Roboflow with [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
    "\n",
    "# Annotate\n",
    "\n",
    "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/roboflow-annotate.gif)\n",
    "\n",
    "# Version\n",
    "\n",
    "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/robolfow-preprocessing.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Assemble Our Dataset（画像の収集）\n",
    "\n",
    "独自のモデルを学習させるにはバウンディングボックス(囲み枠)で検出してほしい物体の周りを囲った印をつけた（アノテーションされた）表現画像のデータセットが必要です。そしてそれをYolov5のフォーマットで表現しなければなりません。\n",
    "\n",
    "Roboflowでは、二つのやり方から選べます。:\n",
    "\n",
    "* すでにあるデータセットをYOLOv5のフォーマット変換。Roboflowは [30をこえる物体検知フォーマットをサポートします](https://roboflow.com/formats)\n",
    "* 新たに収集したそのままの画像にRoboflow 内でとノテーション実行。 [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
    "<br>訳注：実際にアノテーションは上の[Roboflow Annotate](https://docs.roboflow.com/annotate)に行ってアプリの上でやります。この下に続くコードはYolov5へのフォーマット変換です\n",
    "\n",
    "# Annotate（アノテーション　長方形or多角形ポリゴンが使える）\n",
    "\n",
    "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/roboflow-annotate.gif)\n",
    "\n",
    "# Version\n",
    "\n",
    "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/robolfow-preprocessing.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignore distutils configs in setup.cfg due to encoding errors.\n",
      "WARNING: Ignore distutils configs in setup.cfg due to encoding errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (0.2.14)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (2.4.7)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (9.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (3.5.1)\n",
      "Requirement already satisfied: wget in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (3.2)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (0.9.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (6.0)\n",
      "Requirement already satisfied: chardet==4.0.0 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (1.21.5)\n",
      "Requirement already satisfied: cycler==0.10.0 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (0.10.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (0.20.0)\n",
      "Requirement already satisfied: six in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: idna==2.10 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (2.10)\n",
      "Requirement already satisfied: glob2 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (0.7)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (4.64.0)\n",
      "Requirement already satisfied: urllib3==1.26.6 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (1.26.6)\n",
      "Requirement already satisfied: kiwisolver==1.3.1 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (1.3.1)\n",
      "Requirement already satisfied: certifi==2021.5.30 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (2021.5.30)\n",
      "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (4.6.0.66)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib->roboflow) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib->roboflow) (4.25.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages (from requests->roboflow) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"gskt58GHyGz0SrD1r2p1\")\n",
    "project = rf.workspace(\"7seg\").project(\"7segno\")\n",
    "dataset = project.version(2).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2wGvjd4Z_92",
    "outputId": "9b100d09-654e-4c01-e77d-71d3ec4264c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=ultralytics\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2jjT5uIHo6l5"
   },
   "outputs": [],
   "source": [
    "# set up environment\n",
    "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwJcaoPGF4VI",
    "outputId": "4e22e5ac-6980-403e-a723-ba205596761c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in /content/datasets/American-Mushrooms-1 to yolov5pytorch: 100% [3866359 / 3866359] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to /content/datasets/American-Mushrooms-1 in yolov5pytorch:: 100%|██████████| 278/278 [00:00<00:00, 1046.33it/s]\n"
     ]
    }
   ],
   "source": [
    "#after following the link above, recieve python code with these fields filled in\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"YOUR API KEY HERE\")\n",
    "#project = rf.workspace().project(\"YOUR PROJECT\")\n",
    "#dataset = project.version(\"YOUR VERSION\").download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7yAi9hd-T4B"
   },
   "source": [
    "# Step 3: Train Our Custom YOLOv5 model\n",
    "\n",
    "Here, we are able to pass a number of arguments:\n",
    "- **img:** define input image size\n",
    "- **batch:** determine batch size\n",
    "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
    "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
    "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
    "- **cache:** cache images for faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:  カスタムYOLOv5 modelの学習\n",
    "\n",
    "さあ、これで実行に必要な引数を渡すことができるようになりました。:\n",
    "<br>この次のコードはYolov5の通常の学習用スクリプトです。）Roboflowは関係ない）\n",
    "- **img:** 入力渡される画像数\n",
    "- **batch:** バッチサイズ\n",
    "- **epochs:** 学習エポック数 (メモ: しばしば 3000以上が普通とられます。)\n",
    "- **data:** 用意したデータセットのある場所→次のところにセーブします`dataset.location`\n",
    "- **weights:** 学習結果を始める「重み係数」weightsが入ってるバスの指定。ここでは一般的なCOCOの事前学習モデル を指定している\n",
    "<br>訳注：転移学習の開始元\n",
    "- **cache:** 学習を加速するために画像をキャッシュする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaFNnxLJbq4J",
    "outputId": "cd10b508-268b-4091-e76e-8aa3acf2984b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data={dataset.location}/data.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=150, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m\\u26a0\\ufe0f YOLOv5 is out of date by 14 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 636, in <module>\n",
      "    main(opt)\n",
      "  File \"train.py\", line 504, in main\n",
      "    check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # checks\n",
      "  File \"C:\\Users\\81805\\yolov5\\utils\\general.py\", line 462, in check_file\n",
      "    assert len(files), f'File not found: {file}'  # assert file was found\n",
      "AssertionError: File not found: {dataset.location}/data.yaml\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 416 --batch 16 --epochs 150 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=C:/Users/81805/yolov5/7SEGNo.v13i.yolov5pytorch/data.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=150, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m\\u26a0\\ufe0f YOLOv5 is out of date by 14 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 \\U0001f680 v6.1-386-g2e57b84 Python-3.8.13 torch-1.12.1+cpu CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 \\U0001f680 runs in Weights & Biases\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 \\U0001f680 runs in ClearML\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n",
      "\n",
      "Dataset not found \\u26a0\\ufe0f, missing paths ['C:\\\\Users\\\\81805\\\\valid\\\\images']\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 636, in <module>\n",
      "    main(opt)\n",
      "  File \"train.py\", line 529, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"train.py\", line 109, in train\n",
      "    data_dict = data_dict or check_dataset(data)  # check if None\n",
      "  File \"C:\\Users\\81805\\yolov5\\utils\\general.py\", line 514, in check_dataset\n",
      "    raise Exception('Dataset not found \\u274c')\n",
      "Exception: Dataset not found \\u274c\n"
     ]
    }
   ],
   "source": [
    "#C:\\Users\\81805\\yolov5\\7SEGNo.v13i.yolov5pytorch\n",
    "!python train.py --img 416 --batch 16 --epochs 150 --data C:/Users/81805/yolov5/7SEGNo.v13i.yolov5pytorch/data.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All you have to do is to keep train, test, validation\n",
    "(these three folders contain images and labels), and yolov5 folder\n",
    "(that is cloned from GitHub) in the same directory. \n",
    "Also, another thing is that the 'data.yaml' file has to keep inside the yolov5 folder.\n",
    "'''\n",
    "!python train.py --img 416 --batch 16 --epochs 150 --data ./data.yaml --weights yolov5s.pt --cache\n",
    "#!python train.py --img 416 --batch 16 --epochs 10 --data ./data.yaml --cfg ./models/yolov5m.yaml --weights '' --name yolov5m_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcIRLQOlA14A"
   },
   "source": [
    "# Evaluate Custom YOLOv5 Detector Performance\n",
    "Training losses and performance metrics are saved to Tensorboard and also to a logfile.\n",
    "\n",
    "If you are new to these metrics, the one you want to focus on is `mAP_0.5` - learn more about mean average precision [here](https://blog.roboflow.com/mean-average-precision/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カスタムYOLOv5検出器のパフォーマンスを評価する\n",
    "学習の損失とパフォーマンスのメトリクスはTensorboardとログファイルにセーブされます\n",
    "\n",
    "もしもこれらのメトリクスに不慣れな場合は`mAP_0.5`というのに特化してみてもいいでしょう。平均の精度についてはこちらを参照ください [here](https://blog.roboflow.com/mean-average-precision/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jS9_BxdBBHL"
   },
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "# Launch after you have started training\n",
    "# logs save in the folder \"runs\"\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtmS7_TXFsT3"
   },
   "source": [
    "# Run Inference  With Trained Weights\n",
    "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習で得られた重みを使って推論を実行\n",
    "Roboflowからダウンロードされた事前学習のチェックポイントでtest/imagesフォルダの中身について推論を実行します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWjjiBcic3Vz",
    "outputId": "fc66484e-02b7-4e81-af74-6703dfea7c6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./runs/train/exp/weights/best.pt'], source=0, data=data.yaml, imgsz=[416, 416], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5 \\U0001f680 v6.1-386-g2e57b84 Python-3.8.13 torch-1.12.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7064065 parameters, 0 gradients, 15.9 GFLOPs\n",
      "WARNING: Environment does not support cv2.imshow() or PIL Image.show() image displays\n",
      "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1267: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
      "\n",
      "1/1: 0...  Success (inf frames 640x480 at 30.00 FPS)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"detect.py\", line 257, in <module>\n",
      "    main(opt)\n",
      "  File \"detect.py\", line 252, in main\n",
      "    run(**vars(opt))\n",
      "  File \"C:\\Users\\81805\\anaconda3\\envs\\yolov5\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"detect.py\", line 111, in run\n",
      "    for path, im, im0s, vid_cap, s in dataset:\n",
      "  File \"C:\\Users\\81805\\yolov5\\utils\\dataloaders.py\", line 381, in __next__\n",
      "    if not all(x.is_alive() for x in self.threads) or cv2.waitKey(1) == ord('q'):  # q to quit\n",
      "cv2.error: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1333: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights ./runs/train/exp/weights/best.pt --img 416 --conf 0.5 --source 0 --data data.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZbUn4_b9GCKO",
    "outputId": "70f601b7-d53d-4efd-fb97-6ae373b15b9f"
   },
   "outputs": [],
   "source": [
    "#display inference on ALL test images\n",
    "\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
    "    display(Image(filename=imageName))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8dHcni6CJYt"
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've trained a custom YOLOv5 model to recognize your custom objects.\n",
    "\n",
    "To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n",
    "\n",
    "To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n",
    "\n",
    "Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結論と次のステップ\n",
    "\n",
    "おめでとうございます！カスタムの YOLOv5 modelを学習させて、新しい物体の検知に成功しましたね。\n",
    "\n",
    "あなたのモデルのパフォーマンスを改善するには、データセットの内容に漏れがなく品質を上げることを繰り返すべきです。こちらのガイドをご覧ください [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n",
    "\n",
    "あなたのモデルをアプリケーションにデプロイするには、こちらのガイドをご覧ください [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n",
    "\n",
    "いったんあなたのモデルを本番環境に適用したら、継続的に繰り返しデータセットとモデルを改善したくなるかもしれません。次を参考にしてください [active learning](https://blog.roboflow.com/what-is-active-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "7iiObB2WCMh6",
    "outputId": "5c2c1e8a-a857-4284-d627-42f99724c9e8"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_bd6fc902-d327-4332-bf86-0f81058a7d7f\", \"best.pt\", 14358689)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export your model's weights for future use\n",
    "from google.colab import files\n",
    "files.download('./runs/train/exp/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNn-obvOGITm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "YOLOv5-Custom-Training.ipynb のコピー",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
